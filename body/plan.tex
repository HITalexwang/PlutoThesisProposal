% !Mode:: "TeX:UTF-8"

\section{学位论文的主要内容、实施方案、及可行性论证}

\subsection{主要研究内容}


本课题致力于解决顺滑问题，其主要有三个难点和挑战。一个是长距离依赖问题，这个问题可以从两方面来看，一方面Edit类型的短语块本身长度可能会很长，在English Switchboard数据中，最大的Edit块甚至包含有15个词，另一方面，Edit类型和其对应的修正部分之间并不总是相邻，中间可能隔着很多的词，因此要判断一个词是不是Edit类型，需要看到非常远距离的信息。另一个难点是要保证顺滑后的句子的句法完整性，如果一个句子的关键成分被误认为Edit类型，将会导致非常严重的后果，另一个是非顺滑块和对应的顺滑块之间存在着很紧密的联系，充分利用块之间的相似性将会变得非常重要。另一方面，公开的标注有顺滑现象的数据是十分稀少的，因而，利用来自互联网上的大量的未标注的口语数据（如字幕数据），以及大量的不带不流畅现象的数据（如新闻领域的数据）来帮助顺滑任务，将是一个十分有意义的研究点。


本课题针对上述难点和挑战，结合自然语言处理技术进行了相应的理论证明及实验尝试,其具体内容如下：

(1)
传统的序列标注等方法很难充分的利用全局信息和建模句子的结构完整性。
句法和顺滑的联合方法不但需要额外的句法标注，其性能也受到句法性能的制约，因此，我们提出了一个基于注意力机制的方案。
我们的方法首先会对整个句子进行编码，之后再次序的生成顺滑块，其可以理解为一个全局的决策过程，因此能够充分的利用全局信息，而且其生成过程本身可以理解为一个语言模型的生成过程，其有一定的能力保证句法的结构完整性。实验结果也证明，我们的模型在标准的数据集上取得了较好的实验效果。
（本部分工作已完成，并发表在自然语言处理的重要国际会议COLING2016上）

(2)
块之间的相似性对于顺滑任务是非常重要的，然而之前只有很少的工作尝试显示的去利用块信息。
有工作尝试用semi-CRF模型去建模块信息，但是由于受到马尔可夫假设的约束，其也只能利用局部的块信息。我们的工作尝试利用一种不带句法信息的转移系统来解决顺滑问题。基于转移的方法本身能很好的利用全局信息，所以其能很好的解决长距离依赖问题，而且通过引入一个$stack$来存储被标注为非顺滑的词序列，我们的模型能很好的利用块之间的一些相似性等信息。实验结果也证明，我们的模型在标准的数据集上取得了较好的实验效果。
（本部分工作已完成，并发表在自然语言处理的重要国际会议EMNLP2017上）


(3)
以前的工作基本都是从左往右进行决策的，这种方案的一个主要问题是决策是相对局部的，因为在做当前的决策时，其只能利用左边的决策信息，对右边的部分是未知的。以“i want a flight to bosthon to denver”为例，在决定"to boston"是否应该被删除的时候，我们如果知道"to denver"是一个完整的块，将对决策产生很大的帮助。而且从人的思考角度来看，我们也倾向于先识别出“to boston”，“to denver”这些块，然后再决定删除哪些部分。因此，我们将尝试采用easy-first的方案，主要思路是先识别一些比较容易生成的块，如“to boston”等，然后再去决定删除那些块。easy-first方案由于决策是全局的，而且能充分的利用块信息，以及建模相邻块之间的联系（与语言模型类似），因此其理论上能很好的处理顺滑任务的三个挑战。（本部分工作正在进行中）

(4)
公开的标注有顺滑现象的数据是十分稀少的，因而，利用来自互联网上的大量的未标注的口语数据（如字幕数据），以及大量的不带不流畅现象的书面语数据（如新闻领域的数据）来帮助顺滑任务，将是一个十分有意义的研究点。我们将尝试用对偶学习（dual learning）\citeyqy{he2016dual}的方法来降低模型对有监督数据的依赖。对偶学习的最关键一点在于，给定一个原始任务模型，其对偶任务的模型可以给其提供反馈；同样的，给定一个对偶任务的模型，其原始任务的模型也可以给该对偶任务的模型提供反馈；从而这两个互为对偶的任务可以相互提供反馈，相互学习、相互提高。对于我们的顺滑任务来说，从口语到书面语的转化，以及从标准数据到口语数据的转化可以建模成一个对偶学习的过程，从而通过对偶学习的过程，充分的将大量的口语数据和书面语数据结合起来，降低对有监督数据的依赖。（本部分工作正在进行中）


上述各研究内容的关系如图\ref{plan}所示。
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width = 0.9\textwidth]{phd_theis.pdf}
%	\caption{研究内容概况图}
%	\label{plan}
%	\vspace{-1em}
%\end{figure}



\subsection{实施方案}
由于四个研究点中的前两个已经完成，并且被录用，所以实施方案部分将主要介绍后两个研究点。
研究工作将从如下几个方面展开：
\subsubsection{基于easy-first的方案}
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width = 0.6\textwidth]{easy-first.pdf}
%	\caption{easy-first思想在依存句法分析上的示意图}
%	\label{easy_first}
%	\vspace{-1em}
%\end{figure}
easy-first严格来说并不是一个具体的机器学习模型，其只是一种区别于传统的从左往右进行决策的新思路。在easy-first的决策过程中，对于每一步，都有很多候选项，这些候选项可能来源于句子序列中的不同位置，模型会倾向于选择置信度最高的候选项，然后对模型进行更新。所以其核心思想是优先选择那些最容易做出的决策，这些决策会对后面比较难的决策进行帮助，这个思想也很符合人在面对一些问题的思考方式。

以句法分析为例，传统的模型都是从左往右进行决策的，这种搜索方式在每一步的时候，只能依赖于前面的决策步骤，也就是说只看到了句子前面部分的结构，但是看不到右边还未分析的子句的内部结构。但是采用easy-first的搜索方案\citeyqy{goldberg2010efficient, kiperwasser2016easy}，其往往会优先选择一些比较容易识别的弧，然后再在这些已识别的弧的基础上进行更难的弧的识别，这就摆脱了传统的从左往右决策的只能看到左边信息的缺点。如图\ref{easy_first}所示，里面给出了一个分析“a brown fox jumped with joy”这句话的依存句法结构的过程，我们可以看到其最先识别出的往往是“a brown fox”，“with joy”这些比较容易识别的结构，这些简单的结构会对后面比较复杂的结构识别带来很大的帮助。

受到easy-first句法分析的启发，我们也尝试用easy-first的思路来解决顺滑问题。以“i want a flight to bosthon to denver”为例，模型可以先识别一些比较容易生成的块，如“to boston”等，然后再去决定删除哪些块。easy-first方案由于决策是全局的，而且能充分的利用块信息，以及建模相邻块之间的联系（与语言模型类似），因此其理论上能很好的处理顺滑任务的三个挑战。

对于具体的搜索过程，我们设计了三种具体的动作：
\begin{itemize}
	\item COMBINE：这个动作会将相邻的两个块给拼接成一个块。
	\item DEL-LEFT：该动作会把相邻的块的左边部分给删除掉
	\item DEL-RIGHT：该动作会把相邻的块的右边部分给删除掉
\end{itemize}
表 
\ref{action_sample1}
显示了处理句子“a flight to boston to denver”的动作序列。需要注意的是，对于easy-first方案，其每一步的正确选择并不只有一个，也就是说合理的搜索路径不止一个。
\begin{table}[htbp]
	\caption{对于输入\emph{a flight to boston to denver}的处理过程}
	\label{action_sample1}
	\vspace{-0.5em}
	\centering
	%\small
	%\renewcommand{\arraystretch}{1.1}
	%\begin{tabular}{l{2em}l{4em}l{10em}l{6em}l{13em}}
	\begin{tabular}{c l l }
		\hline
		Step & Action &  Buffer   \\
		\hline\hline
		0 & NO&[a, flight, to, boston, to, denver]  \\
		1 & COMBINE & [a-flight, to, boston, to, denver]  \\
		2 & COMBINE & [a-flight,  to, boston, to-denver]   \\
		3 & COMBINE & [a-flight, to-boston, to-denver]   \\
		4 & DEL-RIGHT & [a-flight,to-denver]   \\
		5 & COMBINE & [a-flight-to-denver]  \\
		\hline
	\end{tabular}
\end{table}


对于具体的模型实现，我们首先要对句子中的每个位置的词进行表示。在这里，我们使用了循环神经网络（Recurrent Neural Net，RNN）对每个词的上下文建模。循环神经网络相比传统的前馈神经网络，引入了定向循环的结构，能够处理前后关联的、不定长的序列输入问题。在传统神经网络模型中，层与层之间是全连接的，但是每层之间的节点是无连接的。在RNN中，因为其特有的定向循环结构，使得隐含层之间的节点不再是无连接的，而是有连接的，即一个隐含层的输入不仅同当前输入有关，还包含上一时刻隐含层的输出。由于传统的RNN模型展开后相当于多层的神经网络，层数对应历史输入数据的个数，层数过多会导致训练参数时梯度消失和历史信息损失的问题。也就是说，越远的序列输入对训练权值所能起到的``影响''越小，所以训练的结果往往偏向于新的信息，即不太能有较长的记忆功能，能够利用的历史信息非常有限。长短时记忆模型（Long Short-Term Memory，LSTM）模型能够解决上述问题。图\ref{lstm}是LSTM的单元结构图。
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width = 0.6\textwidth]{lstm.pdf}
%	\caption{LSTM单元结构图}
%	\label{lstm}
%	\vspace{-1em}
%\end{figure}
在LSTM单元中，设计了专门的记忆单元（memory cell）用于储存历史信息。历史信息的更新和使用分别受三个门的控制——输入门（input gate）、遗忘门（forget gate）和输出门（output gate）。
其中，$x_t$为$t$时刻LSTM单元的输入，$C_t$为$t$时刻LSTM单元的值，$h_t$为t时刻LSTM单元的输出。LSTM单元的更新过程如下：
\begin{align}
& i_t = \sigma (W^{(i)}\cdot[x_t ; h_{t-1}]+b^{(i)})  \\
& f_t = \sigma (W^{(f)}\cdot[x_t ; h_{t-1}]+b^{(f)}) \\
& o_t = \sigma (W^{(o)}\cdot[x_t ; h_{t-1}]+b^{(o)})\\
& \tilde{C}_t = tanh(W^{(c)}\cdot[x_t ; h_{t-1}]+b^{(c)})\\%\displaybreak
& C_t = i_t \odot \tilde{C}_t + f_t \odot C_{t-1}\\
& h_t = o_t \odot tanh(C_t)
\end{align}
这里，$\odot$表示两个向量按位相乘，$W^{(i)}$, $b^{(i)}$, $W^{(f)}$, $b^{(f)}$, $W^{(o)}$, $b^{(o)}$, $W^{(c)}$, $b^{(c)}$是神经网络的参数，$\sigma$表示sigmoid函数：
\begin{equation}
\sigma(z) = \frac{1}{1+e^{-z}}
\end{equation}

用一个双向LSTM对句子进行表示学习之后，我们得到了每个位置的词对应的向量表示。下一步的重点是如何保存解码过程中生成的一个个的局部的块。在我们的方案中，我们尝试利用另外一个LSTM网络对每个块进行建模。一旦执行完COMBINE操作，我们会将相邻的右边的块中的词的表示依次拼接到后面的LSTM表示中去。图\ref{segment}是词表示和块组合的整体框架。

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width = 0.6\textwidth]{segment.pdf}
%	\caption{easy-first方案的词表示和块组合方案}
%	\label{segment}
%	\vspace{-1em}
%\end{figure}

每一步，我们的模型都会选择最高得分的动作$(i,l)$，其中$i$表示第$i$个块，$l$表示对应的动作。训练的目标就是让每一步正确动作的得分高于错误动作的得分。我们采用max-margin作为损失函数，每一步的具体的损失函数为：
   \begin{equation}
   \begin{split}
   max\{0, 1-max_{(i,l) \in {G}}Score(i,l) + max_{(i,l) \in {A\backslash G}}Score(i,l)\}
   %p(z | w) = \prod_{t=1}^{|z|}p(z_t | e_t)\nonumber
   \end{split}
   \end{equation}
   其中$G$表示在当前步骤下所有可能的正确的动作，$A$表示当前步骤下所有可能的动作。$Score(i,l)$表示对应动作的得分。


\subsubsection{基于对偶学习的方案}

很多人工智能的应用涉及两个互为对偶的任务，例如机器翻译中从中文到英文翻译和从英文到中文的翻译互为对偶、语音处理中语音识别和语音合成互为对偶、问答系统中回答问题和生成问题互为对偶。这些互为对偶的人工智能任务可以形成一个闭环，使从没有标注的数据中进行学习成为可能。
对偶学习的最关键一点在于，给定一个原始任务模型，其对偶任务的模型可以给其提供反馈；同样的，给定一个对偶任务的模型，其原始任务的模型也可以给该对偶任务的模型提供反馈；从而这两个互为对偶的任务可以相互提供反馈，相互学习、相互提高。

对于顺滑任务来说，我们的目的是将口语句子中的一些不流畅的部分删除，最终转换成偏书面语风格的句子。这个可以理解为一个原始任务。那么，相对应的对偶任务可以理解为一个书面语到口语的转化过程。对于对偶学习来说，可以把两个对偶的任务叫做智能体。
我们的训练过程可以看作两个智能体互相交流的过程：

\begin{itemize}
	\item 第一个Agent 只懂得口语，可以将口语的信息通过一个有噪声的信道发送给第二个Agent，这个有噪声的信道可以使用我们之前的基于转移的模型将口语转换成
	第二个Agent懂得的书面语言。
	\item 第二个Agent只懂得书面语言，她接收到第一个Agent发来的信息（已经被转化成书面语言），然后她会对该信息进行判断（比如语言模型得分，语句是否完整等），
	注意她并不能准确的判断该信息的正确性，因为原始的信息对她来说是不可见的。之后她将此时的信息通过另一个有噪声的信道再发送回第一个Agent，该信道使用另一个基于转移的生成模型将书面语的信息转换为口语。
	\item 第一个Agent接收到第二个Agent传回来的信息后，就与原始的口语句子进行比对，看两者的一致性。通过这个反馈（feedback），两个Agent都可以知道两个信道（两个转化模型）是否执行的比较好（perform well）以及是否可以相互促进。
	\item 训练也可以从第二个Agent开始。这两个Agent通过这个闭环的过程，根据反馈可以不断的提升两个转化模型的性能。
\end{itemize}
从上述描述中可以看出，尽管这两个Agent都没有与之标记的的标签，但是她们依然可以得到关于这两个模型的反馈，并依据此反馈来提高模型的转化能力。由此也可以看出，我们并不需要大量的训练集（为了加快模型的训练速度，也可以先使用少量的标注有顺滑的数据集）。

我们准备用策略梯度方法来训练我们的模型。策略梯度方法的基本思想非常简单：如果我们在执行某个动作之后，观测到了一个很大的回报，我们就通过调整策略（在当前策略函数的参数上加上它的梯度）来增加这个状态下执行这个动作的概率；相反，如果我们在执行某个动作之后，观测到了一个很小的回报，甚至是负的回报，那么我们就需要调整策略（在当前策略函数的参数上减去它的梯度），以降低在这个状态下执行这个动作的概率。对偶学习的学习过程如图\ref{dual}所示

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width = 0.8\textwidth]{dual.pdf}
	%\includegraphics[]{dual.pdf}
%	\caption{对偶学习的学习过程}
%	\label{dual}
%	\vspace{-1em}
%\end{figure}

对偶学习已经在机器翻译等任务上取得了不错的效果，相关的实验也已经证明只用少量的有监督语料就可以取得非常好的效果。对于顺滑任务来说，如果通过对偶学习确实能减少了对有监督数据的依赖，将是一件非常有意义的工作。这块的更具体的实验方案还需要进一步完善。

\subsection{可行性分析}
\begin{enumerate}
	\item 本人长期关注自然语言处理相关技术、理论等。具有一定的理论基础，已经阅读并整理了大量的相关文献，对国内外研究现状有了较清晰的了解，详细掌握了目前顺滑研究中存在的问题以及主要的研究方向。
	
	\item 我们的初期工作已经发表在COLING2016，EMNLP2017等重要国际学术会议上,其创新性得到了同行的认可。
	
	\item 所在的社会计算与信息检索研究中心经过多年的技术积累，已经基本掌握了自然语言处理中词法、句法、语义、复述以及翻译等多种底层的关键技术。其中在上述各个方面都积累了大量的代码与数据，研究中心的语言技术平台享誉国内，加之本人对顺滑技术有较深刻的理解，这些技术和数据都为完成本课题提供了良好的支持。同时科大讯飞研究院负责标注了大量的中文顺滑数据集，对本论文研究的开展起到了很大的帮助。
	
	
	
	
\end{enumerate}
\section{论文的进度安排与预期目标}

\subsection{进度安排}

2015年9月 - 2016年3月：博士论文选题，参考文献收集。

2016年4月 - 2016年7月：分别完成基于LSTM，LSTM-CRF，和注意力机制框架的顺滑分析与研究，完成论文写作并发表

2016年8月 - 2017年4月：完成基于转移的顺滑分析与研究，完成论文写作并发表

2017年5月 - 2018年3月：完成基于easy-first方案的顺滑分析与研究

2018年4月 - 2018年12月：完成基于dual learning方案的顺滑分析与研究

2019年1月 - 2019年6月：撰写毕业论文，申请毕业答辩。

\subsection{预期目标}
本课题致力于解决顺滑问题。经过前期的探索，我们分别从三个角度，针对顺滑问题的特点，提出了三个方法用于解决顺滑的关键问题。首先，我们提出的LSTM-CRF方案可以有效的避免传统LSTM方案的label偏置问题，取得了比LSTM更好的实验结果。其次，我们首次尝试用生成的方法来解决顺滑问题，这种生成式的方法可以利用更加全局的信息，取得了不错的实验结果。最后，针对非顺滑块和顺滑块之间存在紧密关联这一特点，我们尝试了基于转移的方案来解决顺滑问题，取得了目前最好的效果。前期的工作已经在顺滑问题上取得了目前最好的结果，下一步的工作，我们除了对模型本身进行改进，提出了easy-first方案外，还将尝试利用dual learning的方法来利用大规模的生语料，从而减少对有标注语料的依赖。综合这些工作，我们预期目标为从各个角度去探索和解决顺滑问题，并最终构建一个高效，而且只需要少量标注数据的实用的顺滑系统。

\section{学位论文预期创新点}
与前人已发表的研究成果相比，本学位论文的预期创新点主要有以下几点：

（1）提出了一个利用注意力机制来直接生成顺滑块的方法。传统的序列标注等方法很难充分的利用全局信息，而我们的基于注意力机制的方法首先会对整个句子进行编码，之后再次序的生成顺滑块，其可以理解为一个全局的决策过程，因此能够充分的利用全局信息。

（2）提出了一个基于转移的顺滑方案。顺滑任务的一个主要的特点是非顺滑块和对应的顺滑块之间有很强的相似性，我们设计的基于转移的方案可以很好的利用这些块之间联系，实验结果表明基于转移的方案取得了目前最好的试验效果。

（3）正在实现的基于easy-first的顺滑方案。以前的方案大都是从左往右进行决策，这种方案的主要问题是决策是局部的，而且只能看到很少的右边的信息。我们设计的基于easy-first的方案的生成过程是一个由易到难的过程，避开了从左往右进行决策的问题，而且我们的方案在决策的过程中还能充分的利用块信息。

（4）正在实现的基于dual learning的顺滑方案。以前的方案大部分只关注模型本身的改进，很少有利用大量未标注数据的方案。我们的方案通过正向和逆向两个模型的互相交互，将大量的口语数据和标准的新闻数据引入进来，从而期望能利用更少的语料，取得不错的实验效果。


\section{为完成课题已具备和所需的条件、外协计划及经费}
\begin{enumerate}
	\item 相关工作的理论基础的积累：包括文献整理、平台搭建等工作（已完成）；
	\item 基于注意力机制的顺滑方案：首次从生成的角度来解决顺滑问题（已完成）；
	\item 基于转移的顺滑方案：该方案可以充分的利用不同块之间的信息（已完成）；
	\item 基于easy-first的方案：已完成初步的方案设计和代码编写，进一步的实验结果即将得出；
	\item 基于dual learning的方案：已完成初步的方案设计，即将开始编写代码；
	\item 本论文的课题研究经费充足，能够保证相关工作正常进行。
	\item 本论文的课题研究得到了科大讯飞公司合肥研究院（总部）的鼎力支持。
\end{enumerate}

\section{预计研究过程中可能遇到的困难、问题，以及解决的途径}

\begin{enumerate}
	\item 在dual learning方案中如何约束逆向的生成过程：对于dual learning方案，逆向的生成过程是由顺滑的（不含不流畅块）句子生成含有不流畅块的句子，在理论上，在顺滑句子的基础上添加任何成分都是合理的，但是这样无限制的生成方案可能会带来严重的问题，也就是说正向和逆向不是严格对称的，因此如何消除或者减少这种不对称将是实验过程中需要重点关注的部分。
	\item 训练语料不足问题：由于复杂的神经网络需要大规模的训练语料，现有的English SwitchBoard数据只有八万句左右，当模型变复杂的时候，其规模略小。由于实验室跟讯飞有相关的合作，讯飞方面将负责标注大量的中文语料，从而能很好的验证我们模型在大规模语料上的效果。
	\item 实验结果不理想的问题：实验效果不理想可能是由多种不同的原因造成的。如果遇到这种问题，首先会排查代码是否有bug。其次，会对实验结果进行详细的分析，进而根据实验结果，决定是否对模型进行调整和优化。
\end{enumerate}

\section{发表论文情况}
\begin{enumerate}
	\item \textbf{Shaolei Wang}, Wanxiang Che, Yue Zhang, Meishan Zhang and Ting Liu. Transition-Based Disfluency Detection using LSTMs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). 2017.09. Copenhagen, Denmark.(已录用， CCF排名B类，重要国际会议，poster)
	\item \textbf{Shaolei Wang},  Wanxiang Che and Ting Liu. A Neural Attention Model for Disfluency Detection. In Proceedings of the 26th International Conference on Computational Linguistics (Coling 2016). 2016.12. Osaka, Japan.(已录用， CCF排名B类，重要国际会议，oral)
	\item \textbf{Shaolei Wang},  Wanxiang Che, Yijia Liu and Ting Liu. Enhancing Neural Disfluency Detection with Hand-Crafted Features. In Proceedings of China National Conference on Chinese Computational Linguistics (CCL 2016). 2016.10. Yantai, China.
\end{enumerate}

\clearpage

\newpage