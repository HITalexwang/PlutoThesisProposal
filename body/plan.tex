% !Mode:: "TeX:UTF-8"

\section{学位论文的主要内容、实施方案、及可行性论证}

\subsection{主要研究内容}


本课题致力于解决中文语义依存图分析问题。首先，由于语义依存图分析的概念才提出不久，在该任务上目前无论是基于转移的分析方法还是基于图的分析方法的研究都十分有限，二者的优劣尚未有定论。因此需要同时对这两类方法进行探索，分别设计出分析语义依存图的算法。此外，由于目前中外文语义依存图分析任务的数据都相对较少，而我们使用的基于深度学习的方法对数据量的要求往往很大，因此需要探索利用其他技术提高系统性能的方法，包括结合语义依存图分析任务特点的模型融合技术以及多任务学习技术。

本课题针对上述难点和挑战，结合自然语言处理技术进行了相应的理论证明及实验尝试,其具体内容如下：

\begin{enumerate}
	\item \textbf{基于转移的分析方法}
	\ \ \ \ 目前基于转移的依存分析领域大部分工作的分析目标都是树结构，使用基于转移的方法分析依存图的难点在于如何在转移过程中找到一个词的多个父节点。
	针对这个问题，我们提出了新的转移系统，在找到一个词的父节点的同时不对其进行规约，直到其所有父节点都被找到再对其进行规约，从而实现了对语义依存图的分析。
	同时为了从转移状态中获取更丰富的信息作为预测转移动作的依据，我们在基于Stack LSTM的分类器的基础上提出了两个神经网络模块对转移系统中的缓存和子图进行建模，并将其应用在我们的BS-IT依存图分析系统中。该系统在中文语义依存图数据集上取得了目前最好结果。
	（本部分工作已完成，并分别在CCL 2016和AAAI 2018上发表了两篇文章）
	
	\item \textbf{基于图的分析方法}
	\ \ \ \ 由于依存树具有严格的结构限制（例如每个词只能拥有一个父节点），传统基于图的分析方法往往在计算出所有可能子结构的分数的基础上，使用\citeayu{eisner1996three}提出的Eisner算法（一种动态规划算法）进行解码，找出其中的最大生成树。
	然而依存图结构不具备这些结构限制，因此原来的解码算法在该任务上已经不适用。目前该方向的工作普遍采取了对每个子结构逐个判断的方法，并同时使用多种分解方式获得不同级别的子结构，然后用$AD^3$算法解码，解决子结构之间存在重叠的问题。由于没有了结构限制，仅靠第一步计算的分数作为判断依据产生的依存图精度会受到很大影响。
	因此我们首先将尝试将更适合的神经网络引入第一步计算子结构分数的过程中，提高其对子结构分数估计的准确性。同时我们还将尝试使用更高阶（即每个子结构包含更多弧）、更具有图结构特性的子结构分解方式，从而提高最终得到的语义依存图的精度。
	（本部分工作正在进行中）
	
	\item \textbf{基于多TS-LSTM的分析方法}
	\ \ \ \ 模型融合技术在依存分析领域内已经被一些工作证明是一种简单而有效的提升系统性能的方式，但目前普遍被使用的模型融合技术要么是用不同的系统预测出的多个结果进行投票获得最终结果，要么是在同一系统的训练中使用不同的随机初始化种子得到多个模型，然后在预测时同时使用这些模型。前者虽然有效且具有可解释性，但是要求设计多个不同且有效的分析系统，难度较大，往往被用于合并多个前人已经实现的系统。后者虽然只需要设计一个系统，但是可解释性不强。而且这两种方法都没有利用语义依存图的特性。
	因此我们将利用语义依存图短、中、长距离依赖关系的不同，尝试使用窗口长度不同的时间滑动长短时记忆网络（Temporal Sliding LSTM, TS-LSTM）对它们分别进行建模，然后对这些模型进行融合。该方法不仅利用了语义依存图的特性，而且具有较强可解释性。
	该方法可同时应用于基于转移的和基于图的分析方法中，帮助提高系统性能。
	（本部分工作正在进行中）
	
	\item \textbf{基于多任务学习的分析方法}
	\ \ \ \ 由于语义依存图分析任务刚提出不久，相关数据资源有限，目前已知的数据集在中文上只有SemEval-2016 Task 9中文语义依存图数据集，共有25430句人工标注的来自新闻和课文的中文句子。
	英文上只有SemEval-2015 Task 18广义语义依存图数据集，共有38916句人工标注的来自宾州树库华尔街日报部分的英文句子。
	%中文10068+15362 英文35657+1410+1849
	因此在中文语义依存图分析的研究中，更应该有效使用现有所有数据，同时利用句法依存分析、语义角色标注等其它领域数据帮助提高语义依存图分析器精度。
	我们将使用多任务学习技术，尝试把句中词的表示向量输入LSTM中，然后使用LSTM的隐层输出作为每个词的表示。然后将这些表示分别应用到语义依存图分析及相关的其它任务的模型中。这些模型在训练过程中会同时对共用的LSTM表示层进行更新，从而获得一个既具有语义信息，又具有句法等其它任务信息的表示层，最终实现同时提高语义依存图和其他相关任务分析器性能的目的。
	该方法可同时应用于基于转移的和基于图的分析方法中，帮助提高系统性能。
	（本部分工作正在进行中）
	
\end{enumerate}

上述各研究内容的关系如图\ref{fig:relation}所示。
\begin{figure}[htbp]
	\centering
	\includegraphics[width = 120mm]{picture/relation.jpg}
	\caption{研究内容概况图}
	\label{fig:relation}
\end{figure}



\subsection{实施方案}

由于四个研究点中的第一个已经完成，并且被录用，所以实施方案部分将主要介绍另外三个研究点。
研究工作将从如下几个方面展开：

\subsubsection{基于图的分析方法}

基于图的分析方法一般分为两步，第一步计算完全图中每个子结构的分数，第二步选出分数之和最大的子结构的组合作为预测出的目标结构。根据子结构的复杂度，基于图的分析模型可以分为一阶\citeyqy{mcdonald-crammer-pereira:2005:ACL}、二阶\citeyqy{mcdonald2006online}和高阶模型\citeyqy{koo-collins:2010:ACL}，这里的阶数指的是每个子结构中依存弧的个数（例如一阶模型中每个子结构只包括一条弧）。高阶模型可以使用更复杂的子结构特征，因此其准确率更高，但解码算法的效率也会降低。

目前基于图的依存分析领域大部分工作的分析目标都是树结构的，由于依存树具有严格的结构限制（例如每个词只能有一个父节点），这些方法往往能够使用Eisner算法\citeyqy{eisner1996three}或在其基础上衍生出的其它动态规划算法\citeyqy{koo-collins:2010:ACL}进行解码。但是依存图结构中没有这些结构限制，因此上述方法在此不再适用。目前该方向的工作普遍采取了对每个子结构逐个判断的方法，这种方法虽然简单直观，但由于没有了结构限制，仅靠第一步计算的分数作为判断依据产生的依存图精度会受到很大影响。在此情况下要设计精度更高的基于图的依存分析方法，主要有两条路径，一是提高第一步中对每个子结构分数估计的准确性，二是选取更适合图结构的高阶模型，使得子结构本身就具有图的一定特性，从而提高最终得到的语义依存图的精度。接下来我们将分别介绍这两条路径的实施方案。

为了便于理解，我们首先以一阶模型为例，给出基于图的依存分析中第一步计算依存结构分数的形式化定义。在该模型中，一个依存结构的分数$s(\boldsymbol{x},\boldsymbol{y})$为其中所有子结构的分数之和（即所有弧的分数之和）：

\begin{equation}
s(\boldsymbol{x},\boldsymbol{y})=\sum_{(i,j) \in \boldsymbol{y}} s(i,j)
\end{equation}

其中，$\boldsymbol{x}=x_1,\dots,x_n$表示输入句子，$\boldsymbol{y}$表示其对应的依存结构。我们可以将$\boldsymbol{y}$看作一个依存弧集合，因此$(i,j) \in \boldsymbol{y}$表示从词$x_i$指向词$x_j$的弧在$\boldsymbol{y}$中。

目前以依存图为分析目标的工作中，大多使用传统的基于离散特征的模型，使用高维稀疏的特征向量与特征权重向量的点积计算子结构的分值：

\begin{equation}
s(i,j) = \mathbf{w} \cdot \mathbf{f}(i,j)
\end{equation}

其中$\mathbf{f}(i,j)$表示从$x_i$指向$x_j$的弧对应的高维二元特征向量，$\mathbf{w}$表示特征权重向量。$\mathbf{f}(i,j)$中的每一维$f(i,j)$都用0和1表示一个特征是否出现，例如：

\begin{equation}
f(i,j)=
\begin{cases}
1& x_i = \text{"我" 且\ } x_j = \text{"是"} \\
0& \text{其他情况}
\end{cases}
\end{equation}

\citeayu{peng-thomson-smith:2017:Long}将依存句法分析领域中使用神经网络计算子结构分数的想法\citeyqy{kiperwasser2016simple}引入语义依存图分析中来，将句中词的词向量、词性向量作为双向LSTM的输入，然后将正向和反向LSTM的第$i$个时间节点的隐层状态拼接起来作为词$x_i$的表示向量$\mathbf{h}_i=[\overrightarrow{\mathbf{h}}_i;\overleftarrow{\mathbf{h}}_i]$。用这些低维致密的表示向量代替原有高维稀疏特征向量计算子结构分数，证明了该方法在依存图分析中的有效性。

\citeayu{peng-thomson-smith:2017:Long}的方法虽然成功将神经网络引入语义依存图分析任务中，但他们的对于词的表示比较单一，没有考虑一个词分别作为弧中的父节点和子节点时的不同情况。
目前基于图的句法分析方法中性能最好的是\citeayu{dozat2017deep}提出的基于深层双仿射（Biaffine）神经网络的分析器，他们凭借该分析器在CoNLL 2017多语言依存句法分析任务\citeyqy{zeman-EtAl:2017:K17-3}上取得了世界第一的成绩\citeyqy{dozat-qi-manning:2017:K17-3}。
该方法获得句中词的表示向量的步骤与\citeayu{peng-thomson-smith:2017:Long}相同，但在之后将这些表示输入4个不同的MLP分别获得每个词的4个表示向量：

\begin{equation}
\label{eq:arc-dep}
\mathbf{h}^{(arc-dep)}_i = \text{MLP}^{(arc-dep)}(\mathbf{h}_i)
\end{equation}
\begin{equation}
\label{eq:arc-head}
\mathbf{h}^{(arc-head)}_i = \text{MLP}^{(arc-head)}(\mathbf{h}_i)
\end{equation}
\begin{equation}
\label{eq:rel-dep}
\mathbf{h}^{(rel-dep)}_i = \text{MLP}^{(rel-dep)}(\mathbf{h}_i)
\end{equation}
\begin{equation}
\label{eq:rel-head}
\mathbf{h}^{(rel-head)}_i = \text{MLP}^{(rel-head)}(\mathbf{h}_i)
\end{equation}

其中$\mathbf{h}^{(arc-dep)}_i$和$\mathbf{h}^{(arc-head)}_i$分别表示$x_i$在计算弧的分数时作为子节点和作为父节点时的表示向量，$\mathbf{h}^{(rel-dep)}_i$和$\mathbf{h}^{(rel-head)}_i$分别表示$x_i$在计算弧标签分数时作为子节点和作为父节点时的表示向量。在此基础上利用双仿射分类器计算句中每个词作为词$x_i$的父节点时该弧的分数：

\begin{equation}
\begin{split}
\mathbf{s}^{(arc)}_i & = H^{(arc-head)}W^{(arc)}\mathbf{h}^{(arc-dep)}_i \\
& + H^{(arc-head)}\mathbf{b}^{\top(arc)}
\end{split}
\end{equation}

其中$\mathbf{s}^{(arc)}_i$是每个词作为$x_i$的父节点的分数向量，它的第$j$维表示弧$(j\rightarrow i)$的分数。$H^{(arc-head)} \in \mathbb{R}^{n \times d}$是句中$n$个词的$(arc-head)$表示向量拼接起来的矩阵。$W^{(arc)} \in \mathbb{R}^{d \times d}$是弧的权重矩阵，$\mathbf{h}^{(arc-dep)}_i$是词$x_i$的$(arc-dep)$表示向量，$\mathbf{b}^{(arc)}$是弧的偏移向量。使用这种方法，我们能够计算出任意两个词之间的弧的分数，在此基础上使用解码算法就能得到预测出的依存图结构。之后需要为每条弧计算出对应的弧标签，假设$y'_i$是我们预测出的词$x_i$的父节点，这条弧上弧标签的分数向量为：

\begin{equation}
\begin{split}
\mathbf{s}^{(rel)}_i & = \mathbf{h}_{y'^{(arc)}}^{\top(rel-head)}\mathbf{U}^{(rel)}\mathbf{h}^{(rel-dep)}_i \\
& + W^{(rel)}([\mathbf{h}^{(rel-dep)}_i; \mathbf{h}^{(rel-head)}_{y'^{(arc)}_i}]) \\
& + \mathbf{b}^{(rel)}
\end{split}
\end{equation}

其中$\mathbf{s}^{(rel)}_i \in \mathbb{R}^{k}$是$k$个弧标签的分数向量，每一维表示一个标签的分数。$\mathbf{h}_{y'^{(arc)}}^{(rel-head)}$是词$y'_i$的$(rel-head)$表示向量，$\mathbf{h}^{(rel-dep)}_i$是词$x_i$的$(rel-dep)$表示向量，$\mathbf{U}^{(rel)} \in \mathbb{R}^{d\times k \times d}$是弧标签的权重张量，$W^{(rel)} \in \mathbb{R}^{k \times 2d}$是弧标签的权重矩阵，$\mathbf{b}^{(rel)}$是弧标签的偏置向量。最终选择分数最高的弧标签作为这条弧上的标签$y'^{(rel)}_i$：

\begin{equation}
y'^{(rel)}_i = \arg \max_j s^{(rel)}_{ij}
\end{equation}

该方法不仅有效利用了神经网络强大的信息抽取能力，而且考虑了每个词作为弧中的父节点和子节点时的不同情况，因此在句法依存树分析任务中获得了目前最好结果。为了进一步提高基于图的语义依存图分析方法中第一步对每个子结构分数估计的准确性，我们会将该模型应用到依存图分析任务中来，为第二步的解码提供良好的基础。

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=100mm]{picture/parts.jpg}
	\caption{\citeayu{martins2014priberam}使用的子结构}
	\label{fig:parts}
\end{figure}

为了提高语义依存图分析的精度，我们还会尝试选取更适合图结构的高阶模型，使子结构具有图的一定特性。目前已有的基于图的语义依存图分析研究工作中，一般使用的是一阶或二阶模型。\citeayu{martins2014priberam}在其工作中首次使用了具有图特性的子结构，他们所使用的的所有子结构如图~\ref{fig:parts}所示。其中第一行三个为一阶分解，第二和第三行为二阶分解，随机双父节点和连续双父节点两种子结构为图结构所特有的。在依存句法分析领域，很多工作使用了超过二阶的模型，例如\citeayu{koo-collins:2010:ACL}和\citeayu{martins-almeida-smith:2013:Short}都使用了每个子结构包含三条弧的三阶模型。因此，我们将尝试将上述的随机双父节点和连续双父节点两种子结构扩展到三阶，即随机三父节点和连续三父节点，从而增强子结构的图特性，提高分析系统的精度。

\subsubsection{基于TS-LSTM的分析方法}

模型融合技术目前已经在依存分析领域内取得了一些成功，被视为是一种简单而有效的提升系统性能的方式。但目前普遍被使用的模型融合技术要么是用不同的系统预测出的多个结果进行投票决定最终结果\citeyqy{du-EtAl:2015:SemEval}，要么是在同一系统的训练中使用不同的随机初始化种子得到多个模型，然后在预测时同时使用这些模型\citeyqy{kuncoro-EtAl:2016:EMNLP2016,che-EtAl:2017:K17-3}。前者虽然有效且具有可解释性，但是要求设计多个不同且有效的分析系统，难度较大，往往被用于合并多个前人已经实现的系统。后者虽然只需要设计一个系统，但是可解释性不强。而且这两种方法都没有利用语义依存图的特性。

在语义依存图中，一个重要的区分依存弧的指标就是弧中两个词的距离。基于这一点观察，我们希望按照依存距离的不同建立多个模型，用它们分别对短、中、长距离的依存关系进行建模，然后对这些模型进行融合。因此，我们需要一个能够对可变长度序列进行编码的模型。

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=130mm]{picture/ts-lstm.jpg}
	\caption{TS-LSTM网络结构示意图（Bi-LSTM个数$N=3$，窗口长度$W=4$，时间跨度$TS=2$）}
	\label{fig:ts-lstm}
\end{figure}

我们从\citeayu{lee2017ensemble}的动作识别的研究工作中得到了启发。他们提出了一种时间滑动长短时记忆网络（TS-LSTM），由多个在输入序列上以一定的时间跨度滑动的LSTM组成。通过改变时间跨度$TS$和LSTM的窗口长度$W$可以实现提取不同长度依存信息的目的。由于他们是对动作序列进行建模，需要时序关系，因此只使用了单向LSTM。为了同时获得句中词的上下文信息，我们将会使用双向LSTM，修改后的TS-LSTM的网络结构如图~\ref{fig:ts-lstm}所示，图中的TS-LSTM包括3个窗口长度为$W=4$的双向LSTM（分别用n=0,1,2表示），时间跨度为$TS=2$。其中$\mathbf{x}_t$表示句中第$t$个词的向量表示，$\mathbf{h}^n_t$表示句中第$t$个词在第$n$个Bi-LSTM中的表示。

TS-LSTM中第$n$个Bi-LSTM中正向LSTM的第$t$个时间节点的记忆单元$\mathbf{c}^{f,n}_t$、输入门$\mathbf{i}^{f,n}_t$、遗忘门$\mathbf{f}^{f,n}_t$、输出门$\mathbf{o}^{f,n}_t$及隐层状态$\mathbf{h}^{f,n}_t$分别为：

\begin{equation}
\mathbf{i}^{f,n}_t = \sigma(W^{f,n}_{ix} \mathbf{x}^{f,n}_t + W^{f,n}_{ih} \mathbf{h}^{f,n}_{t-1} + W^{f,n}_{ic} \mathbf{c}^{f,n}_{t-1} + \mathbf{b}^{f,n}_i)
\end{equation}

\begin{equation}
\mathbf{f}^{f,n}_t = \sigma(W^{f,n}_{fx} \mathbf{x}^{f,n}_t + W^{f,n}_{fh} \mathbf{h}^{f,n}_{t-1} + W^{f,n}_{fc} \mathbf{c}^{f,n}_{t-1} + \mathbf{b}^{f,n}_f)
\end{equation}

\begin{equation}
\mathbf{c}^{f,n}_t = \mathbf{f}^{f,n}_t \mathbf{c}^{f,n}_{t-1} + \mathbf{i}^{f,n}_t \tanh(W^{f,n}_{cx}\mathbf{x}^{f,n}_t + W^{f,n}_{ch}\mathbf{h}^{f,n}_{t-1} + \mathbf{b}^{f,n}_c)
\end{equation}

\begin{equation}
\mathbf{o}^{f,n}_t = \sigma(W^{f,n}_{ox} \mathbf{x}^{f,n}_t + W^{f,n}_{oh} \mathbf{h}^{f,n}_{t-1} + W^{f,n}_{oc} \mathbf{c}^{f,n}_{t-1} + \mathbf{b}^{f,n}_o)
\end{equation}

其中$\sigma(\cdot)$表示sigmoid函数，$W^{f,n}_{ab}$表示第$n$个Bi-LSTM的正向LSTM的向量$\mathbf{b}$和$\mathbf{a}$之间的权重矩阵，$\mathbf{b}^{f,n}$表示第$n$个Bi-LSTM的正向LSTM中的偏置向量。反向LSTM的公式与其上述相同，只是方向相反，这里不再赘述。第$n$个Bi-LSTM的反向LSTM的在$t$时刻的隐层状态为$\mathbf{h}^{b,n}_t$，第$n$个Bi-LSTM中第$t$个词最终的表示向量$\mathbf{h}^n_t = [\mathbf{h}^{f,n}_t;\mathbf{h}^{b,n}_{W^n-t+1}]$，$W^n$是第$n$个Bi-LSTM的窗口长度。
使用TS-LSTM，通过改变时间跨度和LSTM窗口长度，就可以实现对语义依存图中的短、中、长距离依赖的分别建模，该结构如图~\ref{fig:multi-ts-lstm}所示。该模型可以同时应用到基于转移的和基于图的依存分析中。

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=78mm]{picture/multi-ts-lstm.jpg}
	\caption{使用多个TS-LSTM对输入序列建模}
	\label{fig:multi-ts-lstm}
\end{figure}

在基于转移的依存分析中，可以使用该模型对缓存$\beta$进行建模，用时间跨度和Bi-LSTM窗口长度不同的TS-LSTM获得缓存基于不同距离依赖的表示向量，然后将它们拼接起来作为缓存最终的表示向量。具体来说，首先计算第$l$个TS-LSTM的表示向量$\mathbf{q}^l$：

\begin{equation}
\mathbf{q}^l = \sum^{W_l-1}_{t=0} \text{concat}([\mathbf{h}^{n}_{n\cdot TS_l + t}]^{n=N_l-1}_{n=0},0)
\end{equation}

其中$W_l$、$TS_l$和$N_l$分别是第$l$个TS-LSTM的窗口大小、时间跨度和Bi-LSTM个数，$[(\cdot)_n]^{n=N_l-1}_{n=0} = [(\cdot)_0;(\cdot)_1;\dots;(\cdot)_{N_l-1}]$，$\text{concat}([\dots],0)$表示把$[\ ]$中的元素沿第$0$个坐标轴拼接起来。这些基于不同长度依赖的TS-LSTM的表示向量既可以分别作为当前缓存$\beta$的表示替换公式~\ref{eq:trans}中的$\mathbf{b}_t$获得多个模型，然后对这些模型进行融合，也可以拼接起来后再替代原来的缓存在时刻$t$的表示向量$\mathbf{b}_t$：

\begin{equation}
\mathbf{b}_t = [\mathbf{q}^S;\mathbf{q}^M;\mathbf{q}^L;\mathbf{q}^W]
\end{equation}

其中$\mathbf{q}^S$、$\mathbf{q}^M$、$\mathbf{q}^L$和$\mathbf{q}^W$分别表示短、中、长距离依赖和整句TS-LSTM的表示向量。

在基于图的依存分析中，用第$l$个TS-LSTM，可以计算出句中第$t$个词的表示$\mathbf{h}^l_t$：

\begin{equation}
\mathbf{h}^l_t = \sum_{n \in [(t-W_l/)TS_l,t/TS_l]}\mathbf{h}^{l,n}_t
\end{equation}

其中$\mathbf{h}^{l,n}_t$表示第$l$个TS-LSTM中第$n$个Bi-LSTM计算出的句中第$t$个词的表示。因此，对每个词可以获得分别来自短、中、长距离依赖及整句TS-LSTM的4个表示向量。分别用这4组表示向量替换公式~\ref{eq:arc-dep}-\ref{eq:rel-head}中的$\mathbf{h}_i$，可以得到4个基于不同距离依赖的TS-LSTM的模型。然后可以用传统模型融合方法对这4个模型进行融合，从而提高基于图的依存图分析系统的性能。

\subsubsection{基于多任务学习的分析方法}

多任务学习（Multi-Task Learning, MTL）是一种通过利用相关任务中包含的信息帮助目标任务学习的归纳迁移方法，它通过使用共享的表示平行地学习多个任务来实现这一目标\citeyqy{caruana1998multitask}。自然语言处理领域早已有一系列将多任务学习应用于分词、词性标注及句法分析等任务的工作\citeyqy{li-EtAl:2011:EMNLP3,hatori-EtAl:2012:ACL2012,bohnet-nivre:2012:EMNLP-CoNLL}。这类工作基本都可以归纳为使用了\citeayu{ando2005framework}提出的参数共享方法。在神经网络模型框架下，\citeayu{Collobert:2008:UAN:1390156.1390177}提出了一种通过共享词向量同时解决多个自然语言处理任务的方法。\citeayu{luong2015iclr}将多任务学习技术应用到序列到序列任务中，同时解决句法分析、机器翻译及图像标注等任务。\citeayu{guo2016coling}提出了一个以多种树库为分析目标的多任务学习框架，通过神经网络中多级别的参数共享实现了在多语言、多标注规范的不同树库上的多任务学习。

为了解决语义依存图分析任务目前阶段相关数据资源有限的问题，我们计划在前文中提到的基于转移的和基于图的分析方法中的神经网络框架的基础上使用多任务学习技术。同时利用现有中英文语义依存图数据集以及语义角色标注、句法依存分析等与语义依存图分析任务在结构上相似的任务的数据\citeyqy{surdeanu-EtAl:2008:CONLL,hajivc-EtAl:2009:CoNLL-2009-ST}，从而提高语义依存图分析器的性能。下面分别介绍在基于转移的和基于图的两类依存图分析方法中应用多任务学习技术的方案。我们计划使用以下三个数据集通过多任务学习的方法帮助中文语义依存图分析任务：SemEval-2015 Task 18英文广义语义依存图数据集\citeyqy{oepen-EtAl:2015:SemEval}、中文宾州句法树库（Penn Chinese Treebank, CTB）\citeyqy{xue2005penn}和中文命题库（Chinese Proposition Bank,CPB）\citeyqy{xue2003annotating}。

\begin{figure}[hbtp]
	\centering
	\includegraphics[width=120mm]{picture/mtl.jpg}
	\caption{基于转移的方法中的多任务学习框架}
	\label{fig:mtl}
\end{figure}

在基于转移的方法中，我们将借鉴\citeayu{guo2016coling}的多任务学习框架。他们以\citeayu{ballesteros-dyer-smith:2015:EMNLP}的基于词的Stack-LSTM系统为基础模型，根据所处理多个任务的不同情况（分为多语言标注体系相同和单语言标注体系不同两种情况）设计了不同的参数共享策略。我们将多任务学习方法建立在我们提出的BS-IT分析系统基础上，其框架如图~\ref{fig:mtl}所示。不同任务的模型结构相同，但最终预测的转移动作是由该任务的转移系统决定的。对于语义依存图分析任务，我们使用\ref{approach}节介绍的转移系统，而对依存句法分析和语义角色标注则使用已有研究工作中的基于转移的方法中的转移系统\citeyqy{nivre2008algorithms,choi2011transition}。除了一般的共享参数之外，每个任务有一个表示其特征的任务向量$\mathbf{t}^n$，该向量首先被加入公式~\ref{eq:trans}中计算转移状态的表示，随后被加入公式~\ref{eq:trans-softmax}中计算每个转移动作的概率：

\begin{equation}
\mathbf{e}_t=\max(0,W[\mathbf{s}_t;\mathbf{b}_t; \mathbf{p}_t; \mathbf{a}_t;\mathbf{t}^n ]+\mathbf{d})
\end{equation}

\begin{equation}
p(z_t|e_t)=\frac{\exp(\mathbf{g}^T_{z_t}[\mathbf{e}_t;\mathbf{t}^n] + q_{z_t})}{\sum_{z'\in A(\sigma, \beta)}\exp (\mathbf{g}^T_{z'}[\mathbf{e}_t;\mathbf{t}^n]+q_{z'})}
\end{equation}

在我们的研究中，将中文语义依存图分析作为目标任务，而其他包括英文语义依存图分析、依存句法分析、语义角色标注等作为相关任务。当相关任务语言为英文时，我们使用\citeayu{guo-EtAl:2015:ACL-IJCNLP2}及\citeayu{guo2016aaai}提出的计算多语言词向量的方式获得中英共用的词向量。我们的参数共享策略中共享的参数包括在训练过程中更新的词向量$E_{word}$、计算子图的Tree-LSTM、计算缓存$\beta$表示的Bi-LSTM、计算栈$\sigma$、双向队列$\delta$的LSTM，以及$\beta$、$\sigma$、$\delta$的表示向量与转移状态的表示向量之间的转换参数矩阵。其他参数则是每个任务独立的。

在基于图的方法中，\citeayu{peng-thomson-smith:2017:Long}首先验证了多任务学习方法的有效性，他们试验了两种多任务学习方法，第一种与上述方法类似，是通过共享底层参数实现多任务学习，第二种利用了英文广义语义依存图数据集上对同一部分英文句子进行了三种规范的标注的情况，在分析时同时利用三种标注中的信息。第二种方法是针对SemEval-2015 Task 18数据集的特点设计的，不具备普适性，因此我们将主要尝试使用第一种共享底层参数的方法。即共享在训练过程中更新的词向量$E_{word}$以及底层的Bi-LSTM，然后在上层使用每个任务独立的参数预测子结构分数。与他们不同的是我们将相关任务的范围扩展到多语言、多任务，以中文语义依存图分析为目标任务，分别将英文语义依存图分析、中文依存句法分析作为相关任务。在相关任务为语义依存图分析时，对其采用与目标任务相同的解码算法，当相关任务为依存句法分析时，对其采用现有的基于图的依存句法分析解码算法\citeyqy{eisner1996three}。

\subsection{可行性分析}
\begin{enumerate}
	\item 本人长期关注自然语言处理相关技术、理论等。具有一定的理论基础，已经阅读并整理了大量的相关文献，对国内外研究现状有了较清晰的了解，详细掌握了目前中文语义依存图分析中存在的问题以及主要的研究方向。
	
	\item 我们的初期工作已经发表在CCL 2016，AAAI 2018等国内外重要学术会议上,其创新性得到了同行的认可。
	
	\item 所在的社会计算与信息检索研究中心经过多年的技术积累，已经基本掌握了自然语言处理中分词、词性标注、句法依存分析、语义角色标注等多种底层的关键技术。在上述各个方面都积累了大量的代码与数据，研究中心的语言技术平台享誉国内，加之本人对语义依存图分析技术已有较深刻的理解，这些技术和数据都为完成本课题提供了良好的支持。同时目前我实验室正在与北京语言大学合作完善中文语义依存图标注体系，标注更多中文语义依存图数据，这为本论文研究工作的开展起到了很大的帮助。
	
	
\end{enumerate}
\section{论文的进度安排与预期目标}

\subsection{进度安排}

2016年9月 - 2017年3月：博士论文选题，参考文献收集。

2017年4月 - 2017年10月：完成基于转移的语义依存图分析方法研究，完成论文写作并发表

2017年11月 - 2018年6月：完成针对语义依存图特点的模型融合技术的研究

2018年7月 - 2019年4月：完成基于图的语义依存图分析方法研究

2019年5月 - 2019年12月：完成利用多语言、多领域数据帮助中文语义依存图分析的研究

2020年1月 - 2020年6月：撰写毕业论文，申请毕业答辩。


\subsection{预期目标}

本课题致力于解决中文语义依存图分析中的关键问题，其中包括两部分，首先是语义依存图分析方法本身，然后是如何提高这些方法的性能。在前期的探索中，我们利用基于转移的依存分析方法，提出了新的能够分析语义依存图的转移系统，同时设计了新的网络结构，利用LSTM获取转移状态的信息用于转移动作的预测。
这部分工作已经在中文语义依存图问题上取得了目前最好的结果，下一步的工作，我们要沿着依存分析的另一条道路，探索基于图的语义依存图分析方法。同时还要在这两类方法的基础上研究基于TS-LSTM的模型融合技术和利用多语言、多领域数据的多任务学习技术帮助语义依存图分析。
综合这些工作，我们预期目标为分别利用基于转移和基于图的方法解决中文语义依存图分析问题，利用基于TS-LSTM的模型融合技术及多任务学习技术提高这些方法的性能，并比较两类方法在该问题上的优劣，最终构建一个高效且能够利用更丰富的资源的实用的中文语义依存图分析系统。

\section{学位论文预期创新点}
与前人已发表的研究成果相比，本学位论文的预期创新点主要有以下几点：
\begin{enumerate}
	\item 提出了一个新的基于转移的语义依存图分析方法及其中用于预测转移动作的神经网络分类器。传统的依存分析主要分析目标大都是树结构，我们提出的分析方法能够直接产生依存图结构，并且还提出了两种有效的神经网络结构对转移状态中的缓存和子图进行建模，显著提升了分析器性能。
	\item 正在研究的基于图的语义依存图分析方法。此前该方向上的工作没有很好地利用神经网络抽取信息的能力，使用的子结构也最多只有二阶。我们将把目前依存句法分析领域内性能最高的网络结构引入中文语义依存图分析中，同时利用包括更多图结构特征的三阶子结构，从两个方面提升基于图的分析方法的性能。
	\item 正在研究的基于TS-LSTM的依存图分析方法。传统的模型融合方法往往是基于多系统投票或只对同一系统采用不同随机初始化种子进行训练得到多个模型，然后用这些模型共同预测，并没有有效利用语义依存图的特性。我们设计的基于TS-LSTM的模型融合方法能够通过调整TS-LSTM窗口大小对依存图中的短、中、长距离依赖分别建模，然后再进行融合。该方法不仅利用了依存图特点，而且具有很好的可解释性。
	\item 正在研究的基于多任务学习的依存图分析方法。目前语义依存图分析领域的研究工作中对多任务学习方法的利用十分有限，只有一个工作将其用到了基于图的分析方法中，而且只是在有三种标注体系的英文语义依存图数据集上。我们设计的方法能够分别在基于转移和基于图的分析方法中利用多语言、多任务数据，大大扩展了能够帮助目标任务：中文语义依存图分析的相关任务的数据量。
\end{enumerate}

\section{为完成课题已具备和所需的条件、外协计划及经费}
\begin{enumerate}
	\item 相关工作的理论基础的积累：包括文献整理、平台搭建等工作（已完成）；
	\item 基于转移的中文语义依存图分析方法：提出新的以依存图为目标的转移系统及相应神经网络分类器（已完成）；
	\item 我实验室正与北京语言大学邵艳秋教授合作继续进行中文语义依存图标注规范的修订及数据的标注工作，这将为本课题提供更多的研究数据；
	\item 本论文的课题研究经费充足，能够保证相关工作正常进行。
\end{enumerate}


\section{预计研究过程中可能遇到的困难、问题，以及解决的途径}

\begin{enumerate}
	\item 基于图的分析方法中的解码算法问题：由于依存图缺少结构上的限制，传统的以依存树为目标的解码算法在此不适用，现有方法一般是对子结构逐个判断，然后用$AD^3$算法解决子结构重叠问题。而解码算法对基于图的分析方法来说至关重要，因此如何提出更有效的解码算法有待进一步研究。
	\item 训练语料不足问题：我们提出的方法都是基于复杂的神经网络结构的，需要大规模的训练语料，现有的中文语义依存图数据集总共只有不到三万句，对于复杂的网络结构来说显得数据量比较小。我实验室目前正与北京语言大学邵艳秋教授合作，继续中文语义依存图数据的标注，这会为未来的研究提供更多的语料资源。
	\item 实验结果不理想的问题：实验效果不理想可能是由多种不同的原因造成的。如果遇到这种问题，首先会排查代码是否有bug。其次，会对实验结果进行详细的分析，进而根据实验结果，决定是否对模型进行调整和优化。
\end{enumerate}

\section{发表论文情况}
\begin{enumerate}
	\item \textbf{Yuxuan Wang}, Wanxiang Che, Jiang Guo and Ting Liu. A Neural Transition-Based Approach for Semantic Dependency Graph Parsing. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). 2018.02. New Orleans, LA, USA.(已录用， CCF排名A类，重要国际会议，oral)
	\item \textbf{Yuxuan Wang}, Jiang Guo,  Wanxiang Che, and Ting Liu. Transition-Based Chinese Semantic Dependency Graph Parsing. In Proceedings of China National Conference on Chinese Computational Linguistics (CCL 2016，最佳论文奖，oral). 2016.10. Yantai, China.
\end{enumerate}

\clearpage

\newpage